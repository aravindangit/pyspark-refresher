{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd60e6ef-7215-4ba0-8b1c-b57ae017e0dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table workspace.default.orders_extract as\n",
    "select\n",
    "  *\n",
    "from\n",
    "  samples.tpch.orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5d19c6-1241-4666-a76c-6fe37bfa7989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table workspace.default.table1_P\n",
    "select * from workspace.default.orders_extract where o_orderstatus = 'P';\n",
    "\n",
    "create or replace table workspace.default.table1_F\n",
    "select * from workspace.default.orders_extract where o_orderstatus = 'F';\n",
    "\n",
    "create or replace table workspace.default.table1_O\n",
    "select * from workspace.default.orders_extract where o_orderstatus = 'O';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8e3857-a376-412c-b7f5-b32fd68d163a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType\n",
    "from pyspark.sql.functions import col, lit\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import date\n",
    "\n",
    "# Sample data for the updates with customerId, new address, and effective date of the change\n",
    "updates_data = [\n",
    "    (1, 'kambar street', date(2023, 6, 17)),\n",
    "    (3, 'porur street', date(2023, 6, 17)),\n",
    "    (6, 'thuvarnkuruchi street', date(2023, 6, 17)),\n",
    "    (7, 'thiruvanmiyur street', date(2023, 6, 17))\n",
    "]\n",
    "\n",
    "# Define schema for the updates DataFrame\n",
    "updates_schema = StructType([\n",
    "    StructField(\"customerId\", IntegerType(), False),\n",
    "    StructField(\"address\", StringType(), False),\n",
    "    StructField(\"effectiveDate\", DateType(), False)\n",
    "])\n",
    "\n",
    "# Create DataFrame for updates using the sample data and schema\n",
    "updates_df = spark.createDataFrame(updates_data, updates_schema)\n",
    "\n",
    "# Define the Delta table name where customer data is stored\n",
    "delta_table_name = \"customers\"\n",
    "\n",
    "# Check if the Delta table exists in the metastore\n",
    "if not spark.catalog.tableExists(delta_table_name):\n",
    "    # If table does not exist, prepare initial data from updates with current flag set to True and no endDate\n",
    "    initial_data = [(row.customerId, row.address, True, row.effectiveDate, None) for row in updates_df.collect()]\n",
    "\n",
    "    # Define schema for the initial data including current and endDate fields for SCD Type 2\n",
    "    initial_schema = StructType([\n",
    "        StructField(\"customerId\", IntegerType(), False),\n",
    "        StructField(\"address\", StringType(), False),\n",
    "        StructField(\"current\", BooleanType(), False),  # Indicates if this record is the current active record\n",
    "        StructField(\"effectiveDate\", DateType(), False),  # When this record became effective\n",
    "        StructField(\"endDate\", DateType(), True)  # When this record was superseded (null if current)\n",
    "    ])\n",
    "\n",
    "    # Create DataFrame for initial data to seed the Delta table\n",
    "    initial_df = spark.createDataFrame(initial_data, initial_schema)\n",
    "\n",
    "    # Write the initial DataFrame to a new Delta table, overwriting if exists\n",
    "    initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)\n",
    "\n",
    "# Load the existing Delta table as a DeltaTable object for merge operations\n",
    "customersTable = DeltaTable.forName(spark, delta_table_name)\n",
    "\n",
    "# Identify updates where the address has changed for customers currently marked as current\n",
    "newAddressesToInsert = updates_df.alias(\"updates\") \\\n",
    "    .join(customersTable.toDF().alias(\"customers\"), \"customerId\") \\\n",
    "    .where(\"customers.current = true AND updates.address <> customers.address\")\n",
    "\n",
    "# Prepare staged updates for merge:\n",
    "# - For changed addresses, create rows with NULL mergeKey to force insert of new records\n",
    "# - For all updates, create rows with mergeKey as customerId to match existing records\n",
    "stagedUpdates = newAddressesToInsert.selectExpr(\"NULL as mergeKey\", \"updates.*\") \\\n",
    "    .union(updates_df.selectExpr(\"customerId as mergeKey\", \"*\"))\n",
    "\n",
    "# Perform SCD Type 2 merge:\n",
    "# - When matched and current record has different address, mark current record as not current and set endDate\n",
    "# - When not matched, insert new record as current with no endDate\n",
    "customersTable.alias(\"customers\").merge(\n",
    "    stagedUpdates.alias(\"staged_updates\"),\n",
    "    \"customers.customerId = mergeKey\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"customers.current = true AND customers.address <> staged_updates.address\",\n",
    "    set={\n",
    "        \"current\": lit(False),  # Mark old record as no longer current\n",
    "        \"endDate\": col(\"staged_updates.effectiveDate\")  # Set endDate to effectiveDate of new record\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"customerId\": col(\"staged_updates.customerId\"),\n",
    "        \"address\": col(\"staged_updates.address\"),\n",
    "        \"current\": lit(True),  # New record is current\n",
    "        \"effectiveDate\": col(\"staged_updates.effectiveDate\"),\n",
    "        \"endDate\": lit(None)  # No endDate for current record\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "# Display the final state of the Delta table after applying updates\n",
    "display(customersTable.toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf351b4-53df-40c9-8c1f-c974c6e12391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.default.customers"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4811036266381711,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "scd type 2 implemetation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
